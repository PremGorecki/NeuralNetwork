{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Qg01OtauG2S+Iz9VIPg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PremGorecki/NeuralNetwork/blob/main/07_rnn/01_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Podział tekstu na słowa\n",
        "Często w NLP - Natural Language Processing używamy słowa token. Tokenem może być pojedyńczy znak, a także cały wyraz. Wsystko zależy od kontekstu i potrzeb. Apy podzielić tekst na tokeny (w tym przypadku wyrazy) użyjemy funkcji text_to_word_sequence()"
      ],
      "metadata": {
        "id": "DhPHOPvedfFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = 'Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.'\n",
        "\n",
        "tokens = text_to_word_sequence(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "1f09lerLdg1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hash('sieć')"
      ],
      "metadata": {
        "id": "1v5LlpBy4OHL",
        "outputId": "7cb6d454-407b-469a-9759-620de4b3a984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2089742206425063879"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hash('sieć') % 100"
      ],
      "metadata": {
        "id": "BsrUZzHY4Qb7",
        "outputId": "5b1a260a-21c2-4891-ba06-d817bdcb650f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# robi has i modul 100 dla wszystkich naraz\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "words = set(tokens)\n",
        "one_hot_tokens = one_hot(text, round(len(words) * 1.3))\n",
        "one_hot_tokens"
      ],
      "metadata": {
        "id": "-FOC1AcG4VGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import hashing_trick\n",
        "\n",
        "hashing_trick(text, round(len(words) * 1.3), hash_function='md5')"
      ],
      "metadata": {
        "id": "nopk5usL4jbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# num_words - zachowana zostanie określona liczba słów ze względu na częstotliwość\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "E49z2s5E4vG7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = ['Great picture!', 'Nice view', 'Good to see you :)', 'Good picture!', 'Good']\n",
        "\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "tokenizer.index_word"
      ],
      "metadata": {
        "id": "Sw3NkAkl4zHq",
        "outputId": "8c19d404-0df7-43e5-a587-eb9b09ba8d3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'good',\n",
              " 2: 'picture',\n",
              " 3: 'great',\n",
              " 4: 'nice',\n",
              " 5: 'view',\n",
              " 6: 'to',\n",
              " 7: 'see',\n",
              " 8: 'you'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_counts"
      ],
      "metadata": {
        "id": "xiY0rIx142ka",
        "outputId": "c6bb9c56-1f3e-4b64-abb0-799e26f092a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('great', 1),\n",
              "             ('picture', 2),\n",
              "             ('nice', 1),\n",
              "             ('view', 1),\n",
              "             ('good', 3),\n",
              "             ('to', 1),\n",
              "             ('see', 1),\n",
              "             ('you', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.document_count"
      ],
      "metadata": {
        "id": "7E2UA2ly454q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po dopasowaniu Tokenizera do danych treningowych można go użyć do kodowania dokumentów danych treningowych jak i danych testowych.\n",
        "\n",
        "Funkcja texts_to_matrix() tworzy wektor dla każdego dokumentu. Długość wektora jest równa długości unikalnych słów we wszystkich dokumentach"
      ],
      "metadata": {
        "id": "Ig9Nhp1X47tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_matrix(samples)"
      ],
      "metadata": {
        "id": "zah0S1wW48TC",
        "outputId": "83a6dc24-e0af-4dce-b606-a46fbebc593c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}